{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38c8454",
   "metadata": {},
   "source": [
    "# TTP Mapping & Quick Evaluation (Retrieval + LLM)\n",
    "\n",
    "Short notebook to map detection descriptions to MITRE ATT&CK technique IDs using:\n",
    "- Sentence-transformer embeddings + nearest neighbor retrieval\n",
    "- Local LLM (phi-4) to consolidate / re-rank and justify\n",
    "- Precision / Recall / F1@K evaluation (if ground truth provided)\n",
    "\n",
    "## Input Files\n",
    "\n",
    "- `MITRE_TPPs.xlsx` (reference techniques)\n",
    "- `MDE_SampleDetections.xlsx` (detections to label) â€” ground truth in column `MiTRE_TTPs` (not sent to the model)\n",
    "\n",
    "## Flow (Cell Numbers)\n",
    "\n",
    "1. (2) Install deps (run once or skip if already installed)\n",
    "2. (3) Config + direct LLM load (always on)\n",
    "3. (4) Load TTPs & prep text\n",
    "4. (5) Embed TTPs\n",
    "5. (6) Build vector index + retrieval helper\n",
    "6. (7) Load detections (keeps `MiTRE_TTPs` separate)\n",
    "7. (8) LLM consolidation + labeling pipeline\n",
    "8. (9) Evaluation\n",
    "9. (10) Inspect an example\n",
    "\n",
    "## Key Params\n",
    "\n",
    "- Retrieval size: `k_retrieval` arg in `label_detections` (Cell 8).\n",
    "- Output top N: `top_n_final` in `label_detections`.\n",
    "- Evaluation K: variable `K` in evaluation cell (Cell 9).\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The ground truth column `MiTRE_TTPs` is never included in the prompt; it's only used for scoring.\n",
    "- If column names differ, edit the mapping variables after they are auto-detected.\n",
    "- LLM is always used so we can evaluate the full retrieval + reasoning chain.\n",
    "- You can save predictions later by dumping the `predictions` list to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing openpyxl...\n",
      "Installing sentence-transformers...\n",
      "Installing sentence-transformers...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['C:\\\\Users\\\\emvictor\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'sentence-transformers']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mpip_install\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m==\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m'\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m'\u001b[39m, p])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mpip_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenpyxl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence-transformers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtransformers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mDependencies ensured.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpip_install\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py:419\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    418\u001b[39m         cmd = popenargs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['C:\\\\Users\\\\emvictor\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\python.exe', '-m', 'pip', 'install', 'sentence-transformers']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# Install required packages (run once). Comment out if already installed.\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "\n",
    "def pip_install(pkgs):\n",
    "\n",
    "    for p in pkgs:\n",
    "\n",
    "        mod_name = p.split('==')[0].replace('-', '_')\n",
    "\n",
    "        try:\n",
    "\n",
    "            importlib.import_module(mod_name)\n",
    "\n",
    "        except ImportError:\n",
    "\n",
    "            print('Installing', p)\n",
    "\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', p])\n",
    "\n",
    "\n",
    "\n",
    "pip_install([\n",
    "\n",
    "    'pandas', 'openpyxl', 'sentence-transformers', 'scikit-learn', 'transformers', 'torch'\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "print('Core dependencies ensured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & configuration (clean, LLM mandatory)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import os, math, json, torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-4\"\n",
    "\n",
    "print(f\"Loading LLM: {MODEL_NAME} ... (first load may download weights)\")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "try:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        llm_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(\"Could not move model to CUDA:\", e)\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = str(next(llm_model.parameters()).device)\n",
    "\n",
    "print('LLM device:', DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def generate_chat(messages, max_new_tokens: int = 128):\n",
    "\n",
    "    inputs = llm_tokenizer.apply_chat_template(\n",
    "\n",
    "        messages,\n",
    "\n",
    "        add_generation_prompt=True,\n",
    "\n",
    "        tokenize=True,\n",
    "\n",
    "        return_dict=True,\n",
    "\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "    ).to(llm_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    generated_text = llm_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17403200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MITRE TTP reference data\n",
    "TPP_FILE = 'MITRE_TPPs.xlsx'  # Adjust if needed\n",
    "try:\n",
    "    ttp_df = pd.read_excel(TPP_FILE)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f'Could not find {TPP_FILE}. Please place it in the working directory.')\n",
    "\n",
    "print('Columns in TTP file:', list(ttp_df.columns))\n",
    "# Attempt to auto-detect columns\n",
    "col_map = {}\n",
    "def detect(col_names, candidates):\n",
    "    for c in candidates:\n",
    "        for cn in col_names:\n",
    "            if cn.lower() == c.lower():\n",
    "                return cn\n",
    "    for c in candidates:\n",
    "        for cn in col_names:\n",
    "            if c.lower() in cn.lower():\n",
    "                return cn\n",
    "    return None\n",
    "col_map['id'] = detect(ttp_df.columns, ['TechniqueID','ID','TID','Technique Id'])\n",
    "col_map['name'] = detect(ttp_df.columns, ['TechniqueName','Name','Technique'])\n",
    "col_map['desc'] = detect(ttp_df.columns, ['Description','Details','TechniqueDescription'])\n",
    "print('Detected column mapping:', col_map)\n",
    "missing = [k for k,v in col_map.items() if v is None]\n",
    "if missing:\n",
    "    raise ValueError(f'Could not detect columns for: {missing}. Please edit this cell and set col_map manually.')\n",
    "\n",
    "# Clean & prep text field for embedding\n",
    "def prep_text(row):\n",
    "    return ' | '.join([str(row[col_map['id']]), str(row[col_map['name']]), str(row[col_map['desc']])])\n",
    "ttp_df['__text'] = ttp_df.apply(prep_text, axis=1)\n",
    "print('Sample prepared text:', ttp_df['__text'].iloc[0][:200])\n",
    "print('Total TTP records:', len(ttp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed TTPs\n",
    "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'  # small & fast\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "ttp_embeddings = embed_model.encode(ttp_df['__text'].tolist(), batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "print('Embedding shape:', ttp_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NearestNeighbors index (cosine similarity via metric='cosine')\n",
    "nn_index = NearestNeighbors(n_neighbors=25, metric='cosine')  # keep larger n_neighbors internally\n",
    "nn_index.fit(ttp_embeddings)\n",
    "print('Vector index ready.')\n",
    "\n",
    "def retrieve_ttp(query: str, top_k: int = 5):\n",
    "    \"\"\"Return top_k technique rows (with similarity) for a query string.\"\"\"\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True)\n",
    "    distances, indices = nn_index.kneighbors(q_emb, n_neighbors=top_k)\n",
    "    # cosine distance -> similarity = 1 - distance\n",
    "    sims = 1 - distances[0]\n",
    "    rows = []\n",
    "    for sim, idx in zip(sims, indices[0]):\n",
    "        r = ttp_df.iloc[idx]\n",
    "        rows.append({\n",
    "            'TechniqueID': r[col_map['id']],\n",
    "            'TechniqueName': r[col_map['name']],\n",
    "            'Similarity': float(sim),\n",
    "            'Description': r[col_map['desc']]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def build_rag_context(candidates):\n",
    "    \"\"\"Build a concise context block for the LLM from retrieved techniques.\n",
    "    Truncates description to keep prompt size manageable.\"\"\"\n",
    "    lines = []\n",
    "    for i, c in enumerate(candidates, start=1):\n",
    "        desc_snip = str(c['Description'])[:400].replace('\\n', ' ')\n",
    "        lines.append(f\"{i}. {c['TechniqueID']} | {c['TechniqueName']} | {desc_snip}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Quick smoke test (retrieval only)\n",
    "_test = retrieve_ttp('credential dumping from lsass memory', top_k=3)\n",
    "_test[:1]  # show first candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detection dataset (ground truth column MiTRE_TTPs will be excluded from model input)\n",
    "DETECTIONS_FILE = 'MDE_SampleDetections.xlsx'  # Adjust if needed\n",
    "GROUND_TRUTH_COLUMN_NAME = 'MiTRE_TTPs'  # Explicit ground truth column (labels expected from LLM)\n",
    "try:\n",
    "    det_df = pd.read_excel(DETECTIONS_FILE)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f'Could not find {DETECTIONS_FILE}. Place it in working directory.')\n",
    "print('Columns in detection file:', list(det_df.columns))\n",
    "\n",
    "# Ensure ground truth column exists (optional)\n",
    "if GROUND_TRUTH_COLUMN_NAME not in det_df.columns:\n",
    "    print(f\"Warning: Expected ground truth column '{GROUND_TRUTH_COLUMN_NAME}' not found. Evaluation will be skipped unless you set det_map['gt'] manually.\")\n",
    "\n",
    "# Auto-detect ID / Name / Description columns while EXCLUDING the ground truth column\n",
    "candidate_cols = [c for c in det_df.columns if c != GROUND_TRUTH_COLUMN_NAME]\n",
    "\n",
    "# ID column: prefer one containing 'id' but not technique; fallback to first non-gt column\n",
    "id_col = next((c for c in candidate_cols if 'id' in c.lower() and 'technique' not in c.lower()), candidate_cols[0])\n",
    "\n",
    "# Name column\n",
    "name_col = next((c for c in candidate_cols if 'name' in c.lower() and c != id_col), (candidate_cols[1] if len(candidate_cols) > 1 else id_col))\n",
    "\n",
    "# Description column: look for 'desc' or 'detail'; fallback to last non-gt column that's not id/name\n",
    "possible_desc = [c for c in candidate_cols if c not in {id_col, name_col}]\n",
    "desc_col = next((c for c in possible_desc if ('desc' in c.lower()) or ('detail' in c.lower()) or ('description' in c.lower())), (possible_desc[-1] if possible_desc else name_col))\n",
    "\n",
    "# Ground truth column mapping\n",
    "_gt = GROUND_TRUTH_COLUMN_NAME if GROUND_TRUTH_COLUMN_NAME in det_df.columns else None\n",
    "\n",
    "# Final mapping\n",
    "det_map = {\n",
    "    'id': id_col,\n",
    "    'name': name_col,\n",
    "    'desc': desc_col,\n",
    "    'gt': _gt\n",
    "}\n",
    "\n",
    "# Safety: assert we are not using ground truth column as description\n",
    "if det_map['gt'] and det_map['gt'] == det_map['desc']:\n",
    "    raise ValueError(\"Ground truth column was mistakenly selected as description. Please adjust mappings manually.\")\n",
    "\n",
    "print('Detected detection column mapping:', det_map)\n",
    "print('Total detections:', len(det_df))\n",
    "\n",
    "# Preview WITHOUT leaking ground truth into description prompt\n",
    "preview_cols = [det_map['id'], det_map['name'], det_map['desc']] + ([det_map['gt']] if det_map['gt'] else [])\n",
    "det_df[preview_cols].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based labeling: retrieval provides candidate context; LLM must return JSON with technique IDs.\n",
    "\n",
    "import json as _json\n",
    "\n",
    "\n",
    "\n",
    "def llm_rag_label(detection_text: str, k_retrieval: int = 8, max_new_tokens: int = 220):\n",
    "\n",
    "    candidates = retrieve_ttp(detection_text, top_k=k_retrieval)\n",
    "\n",
    "    context_block = build_rag_context(candidates)\n",
    "\n",
    "    system_prompt = (\n",
    "\n",
    "        \"You are a cybersecurity assistant mapping detection descriptions to MITRE ATT&CK technique IDs. \"\n",
    "\n",
    "        \"Use ONLY the provided candidate techniques. If none fit, return an empty list. \"\n",
    "\n",
    "        \"Output strict JSON with keys: technique_ids (array of strings), rationale (short string).\"\n",
    "\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "\n",
    "        f\"Detection Description:\\n{detection_text}\\n\\n\" \\\n",
    "\n",
    "        f\"Candidate Techniques (retrieved):\\n{context_block}\\n\\n\" \\\n",
    "\n",
    "        \"Return JSON ONLY. Example format: {\\\"technique_ids\\\": [\\\"T1059\\\"], \\\"rationale\\\": \\\"Short reason\\\"}\"\n",
    "\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "\n",
    "    ]\n",
    "\n",
    "    reply = generate_chat(messages, max_new_tokens=max_new_tokens)\n",
    "\n",
    "\n",
    "\n",
    "    parsed_ids = []\n",
    "\n",
    "    rationale = ''\n",
    "\n",
    "    try:\n",
    "\n",
    "        start = reply.find('{')\n",
    "\n",
    "        end = reply.rfind('}')\n",
    "\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "\n",
    "            obj = _json.loads(reply[start:end+1])\n",
    "\n",
    "            if isinstance(obj.get('technique_ids'), list):\n",
    "\n",
    "                parsed_ids = [str(x) for x in obj['technique_ids'] if x]\n",
    "\n",
    "            rationale = str(obj.get('rationale', ''))\n",
    "\n",
    "    except Exception as pe:\n",
    "\n",
    "        rationale = f'Failed to parse JSON: {pe}. Raw: {reply[:160]}'\n",
    "\n",
    "\n",
    "\n",
    "    candidate_id_set = {c['TechniqueID'] for c in candidates}\n",
    "\n",
    "    parsed_ids = [cid for cid in parsed_ids if cid in candidate_id_set]\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "\n",
    "        'predicted_ids': parsed_ids,\n",
    "\n",
    "        'rationale': rationale,\n",
    "\n",
    "        'raw_response': reply,\n",
    "\n",
    "        'candidates': candidates\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def run_rag_labeling(k_retrieval: int = 8):\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for _, row in det_df.iterrows():\n",
    "\n",
    "        text = str(row[det_map['desc']])\n",
    "\n",
    "        result = llm_rag_label(text, k_retrieval=k_retrieval)\n",
    "\n",
    "        outputs.append({\n",
    "\n",
    "            'DetectionID': row[det_map['id']],\n",
    "\n",
    "            'PredictedTechniqueIDs': result['predicted_ids'],\n",
    "\n",
    "            'Rationale': result.get('rationale'),\n",
    "\n",
    "            'RawResponse': result.get('raw_response'),\n",
    "\n",
    "            'Retrieved': result['candidates']\n",
    "\n",
    "        })\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "predictions = run_rag_labeling()\n",
    "\n",
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d14753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core classification metric: F1@K (micro only)\n",
    "if det_map.get('gt') is None:\n",
    "    print('No ground truth column detected; skipping F1@K evaluation.')\n",
    "else:\n",
    "    import numpy as np, re\n",
    "    TECH_ID_REGEX = re.compile(r\"T[0-9]{4,5}(?:\\.[0-9]{3})?\")\n",
    "    def extract_ids(val):\n",
    "        if pd.isna(val): return []\n",
    "        text = str(val)\n",
    "        return list({m.group(0) for m in TECH_ID_REGEX.finditer(text)})\n",
    "    gt_lists = det_df[det_map['gt']].apply(extract_ids).tolist()\n",
    "    pred_lists = [p['PredictedTechniqueIDs'] for p in predictions]\n",
    "    K = 3\n",
    "    tp = fp = fn = 0\n",
    "    for gt, pred in zip(gt_lists, pred_lists):\n",
    "        set_gt = set(gt)\n",
    "        set_pred = set(pred[:K])\n",
    "        tp += len(set_gt & set_pred)\n",
    "        fp += len(set_pred - set_gt)\n",
    "        fn += len(set_gt - set_pred)\n",
    "    micro_precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    micro_recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    micro_f1 = 2*micro_precision*micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) else 0\n",
    "    results = {'F1@K_micro': micro_f1, 'K': K, 'NumDetections': len(predictions)}\n",
    "    print('F1@K_micro:', round(micro_f1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5211e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness & Relevance (LLM-as-judge only)\n",
    "\n",
    "\n",
    "import re, json as _j\n",
    "\n",
    "\n",
    "TECH_ID_REGEX = re.compile(r\"T[0-9]{4,5}(?:\\.[0-9]{3})?\")\n",
    "\n",
    "\n",
    "gold_col = det_map.get('gt')\n",
    "\n",
    "\n",
    "records = []\n",
    "for i, row in det_df.iterrows():\n",
    "    raw_gold = str(row[gold_col]) if gold_col and not pd.isna(row[gold_col]) else ''\n",
    "    gt_ids = list({m.group(0) for m in TECH_ID_REGEX.finditer(raw_gold)}) if raw_gold else []\n",
    "    pred_entry = predictions[i]\n",
    "    rec = {\n",
    "        'DetectionID': row[det_map['id']],\n",
    "        'GroundTruthIDs': gt_ids,\n",
    "        'PredictedIDs': pred_entry['PredictedTechniqueIDs'],\n",
    "        'Rationale': pred_entry.get('Rationale') or '',\n",
    "        'RetrievedContext': '\\n'.join([f\"{c['TechniqueID']} | {c['TechniqueName']} | {str(c['Description'])[:200]}\" for c in pred_entry['Retrieved']])\n",
    "    }\n",
    "    records.append(rec)\n",
    "\n",
    "\n",
    "def judge_pair(rationale: str, context: str, detection_text: str):\n",
    "    system = (\"You are evaluating a security detection explanation. Output JSON with faithfulness and relevance in [0,1]. \"\n",
    "              \"Faithfulness: grounded in context. Relevance: addresses the detection text.\")\n",
    "    user = (f\"Detection: {detection_text}\\n\\nContext:\\n{context[:1800]}\\n\\nRationale:\\n{rationale}\\n\\nRespond ONLY as {{'faithfulness': <num>, 'relevance': <num>}}\")\n",
    "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "    try:\n",
    "        reply = generate_chat(messages, max_new_tokens=100)\n",
    "        s = reply.find('{'); e = reply.rfind('}')\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            obj = _j.loads(reply[s:e+1].replace(\"'\", '\"'))\n",
    "            f = float(obj.get('faithfulness')) if obj.get('faithfulness') is not None else None\n",
    "            r = float(obj.get('relevance')) if obj.get('relevance') is not None else None\n",
    "            if f is not None and not (0 <= f <= 1): f = None\n",
    "            if r is not None and not (0 <= r <= 1): r = None\n",
    "            return f, r\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "for rec in records:\n",
    "    f, r = judge_pair(rec['Rationale'], rec['RetrievedContext'], rec['Rationale'])\n",
    "    rec['Faithfulness'] = f\n",
    "    rec['Relevance'] = r\n",
    "\n",
    "\n",
    "import pandas as _pd\n",
    "results_df = _pd.DataFrame(records)\n",
    "faith_avg = float(results_df['Faithfulness'].mean()) if results_df['Faithfulness'].notna().any() else None\n",
    "rel_avg = float(results_df['Relevance'].mean()) if results_df['Relevance'].notna().any() else None\n",
    "\n",
    "\n",
    "evaluation_summary = {\n",
    "    'F1@K_micro': results.get('F1@K_micro') if 'results' in globals() else None,\n",
    "    'Faithfulness_avg': faith_avg,\n",
    "    'Relevance_avg': rel_avg,\n",
    "    'NumDetections': len(results_df)\n",
    "}\n",
    "print('Evaluation Summary (Slim):')\n",
    "print(evaluation_summary)\n",
    "results_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single detection with its RAG context & final predictions\n",
    "idx = 0  # change this index to inspect another detection\n",
    "sample_pred = predictions[idx]\n",
    "det_row = det_df.iloc[idx]\n",
    "print('Detection ID:', det_row.get(det_map['id']))\n",
    "print('Detection Name:', det_row.get(det_map['name']))\n",
    "print('Description:', str(det_row.get(det_map['desc']))[:500])\n",
    "print('Predicted Technique IDs:', sample_pred['PredictedTechniqueIDs'])\n",
    "print('Mode:', sample_pred['Mode'])\n",
    "print('Rationale:', sample_pred.get('Rationale'))\n",
    "if det_map.get('gt'):\n",
    "    print('Ground Truth IDs:', det_row.get(det_map['gt']))\n",
    "print('\\nTop Retrieved Candidates (first 5):')\n",
    "for c in sample_pred['Retrieved'][:5]:\n",
    "    print(f\" - {c['TechniqueID']} ({c['Similarity']:.3f}): {c['TechniqueName']}\")\n",
    "print('\\nRaw LLM Response (truncated):')\n",
    "raw = sample_pred.get('RawResponse')\n",
    "if raw:\n",
    "    print(raw[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notes on Direct Model Loading\n",
    "\n",
    "You are now loading the causal LLM (phi-4) directly in the configuration cell. If you want to disable it (to speed up experimentation with retrieval + evaluation only), set `USE_LLM = False` and re-run from the top.\n",
    "\n",
    "### Troubleshooting\n",
    "- If you encounter out-of-memory errors, try a smaller model or add `torch_dtype=\"bfloat16\"` (if supported) when calling `from_pretrained`.\n",
    "- If you only have CPU available, generation will be slower; consider reducing `max_new_tokens`.\n",
    "- To log raw prompts and responses for auditing, wrap calls to `generate_chat` and append entries to a list or dataframe.\n",
    "\n",
    "### Quick Example (already in code)\n",
    "```python\n",
    "reply = generate_chat([\n",
    "    {\"role\": \"user\", \"content\": \"List 2 common credential dumping techniques.\"}\n",
    "])\n",
    "print(reply)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176065b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196a618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29164038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7204c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6120be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a83b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3153c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b9db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971527a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008662b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcef39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c920b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69fb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddbf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9749b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb018a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a36bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eccb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897a8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3ccc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54026a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c1aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
